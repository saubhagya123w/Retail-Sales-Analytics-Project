{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d2bf5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61f13f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saubh\\AppData\\Local\\Temp\\ipykernel_8988\\1527504177.py:96: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  chunk[\"ORDERDATE\"] = pd.to_datetime(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete: seen=2823 loaded=2823 rejected=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError, OperationalError\n",
    "\n",
    "# ====== CONFIG ======\n",
    "DB_USER = \"root\"\n",
    "DB_PASS = \"1234qwert\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"3306\"\n",
    "DB_NAME = \"retail_analytics\"\n",
    "CSV_CHUNKSIZE = 5000\n",
    "csv_file = r\"C:\\Users\\saubh\\OneDrive\\Desktop\\Retail_Project\\Data1\\sales_data_sample.csv\"\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    \"ORDERNUMBER\",\"QUANTITYORDERED\",\"PRICEEACH\",\"ORDERLINENUMBER\",\"SALES\",\n",
    "    \"ORDERDATE\",\"STATUS\",\"QTR_ID\",\"MONTH_ID\",\"YEAR_ID\",\"PRODUCTLINE\",\"MSRP\",\n",
    "    \"PRODUCTCODE\",\"CUSTOMERNAME\",\"PHONE\",\"ADDRESSLINE1\",\"ADDRESSLINE2\",\n",
    "    \"CITY\",\"STATE\",\"POSTALCODE\",\"COUNTRY\",\"TERRITORY\",\"CONTACTLASTNAME\",\n",
    "    \"CONTACTFIRSTNAME\",\"DEALSIZE\"\n",
    "]\n",
    "\n",
    "# ====== HELPERS ======\n",
    "def make_engine():\n",
    "    url = f\"mysql+mysqlconnector://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4\"\n",
    "    return create_engine(url, pool_pre_ping=True)\n",
    "\n",
    "def hash_row(row: pd.Series) -> str:\n",
    "    s = \"|\".join([str(row.get(c, \"\")) for c in EXPECTED_COLS])\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# ====== MAIN ======\n",
    "def main(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"CSV not found:\", csv_path)\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        engine = make_engine()\n",
    "        connection = engine.connect()\n",
    "    except OperationalError as e:\n",
    "        print(\"MySQL connection failed:\", e)\n",
    "        return 1\n",
    "\n",
    "    ingestion_id = None\n",
    "    try:\n",
    "        # Start ingestion log\n",
    "        start = datetime.datetime.utcnow()\n",
    "        r = connection.execute(text(\"\"\"\n",
    "            INSERT INTO ingestion_logs (run_started_at, source_filename, rows_seen, rows_loaded, rows_rejected, warnings)\n",
    "            VALUES (:started, :fname, 0, 0, 0, :warnings)\n",
    "        \"\"\"), {\"started\": start, \"fname\": os.path.basename(csv_path), \"warnings\": \"\"})\n",
    "        ingestion_id = r.lastrowid\n",
    "\n",
    "        rows_seen = 0\n",
    "        rows_loaded = 0\n",
    "        rows_rejected = 0\n",
    "        warnings = []\n",
    "\n",
    "        # Header check\n",
    "        df_head = pd.read_csv(csv_path, nrows=0, encoding='latin1')\n",
    "        csv_cols = [c.strip() for c in df_head.columns.tolist()]\n",
    "        missing = set(EXPECTED_COLS) - set(csv_cols)\n",
    "        extra = set(csv_cols) - set(EXPECTED_COLS)\n",
    "        if missing:\n",
    "            warnings.append(f\"missing_columns:{sorted(list(missing))}\")\n",
    "        if extra:\n",
    "            warnings.append(f\"extra_columns:{sorted(list(extra))}\")\n",
    "\n",
    "        # Chunked read\n",
    "        for chunk in pd.read_csv(\n",
    "            csv_path,\n",
    "            chunksize=CSV_CHUNKSIZE,\n",
    "            dtype=str,\n",
    "            keep_default_na=False,\n",
    "            na_values=[\"\"],\n",
    "            encoding='latin1'\n",
    "        ):\n",
    "            chunk.columns = [c.strip() for c in chunk.columns]\n",
    "            rows_seen += len(chunk)\n",
    "\n",
    "            # Ensure all expected columns\n",
    "            for c in EXPECTED_COLS:\n",
    "                if c not in chunk.columns:\n",
    "                    chunk[c] = None\n",
    "\n",
    "            # Convert numerics\n",
    "            for num_col in [\"QUANTITYORDERED\",\"PRICEEACH\",\"SALES\",\"MSRP\",\"ORDERLINENUMBER\",\"QTR_ID\",\"MONTH_ID\",\"YEAR_ID\"]:\n",
    "                if num_col in chunk.columns:\n",
    "                    chunk[num_col] = pd.to_numeric(chunk[num_col].replace(\"\", pd.NA), errors='coerce')\n",
    "\n",
    "            # Convert ORDERDATE (robust handling)\n",
    "            if \"ORDERDATE\" in chunk.columns:\n",
    "                chunk[\"ORDERDATE\"] = pd.to_datetime(\n",
    "                    chunk[\"ORDERDATE\"].str.strip(),\n",
    "                    errors=\"coerce\",\n",
    "                    infer_datetime_format=True\n",
    "                ).dt.date\n",
    "\n",
    "            # Metadata\n",
    "            chunk[\"_source_filename\"] = os.path.basename(csv_path)\n",
    "            chunk[\"_ingested_at\"] = datetime.datetime.utcnow()\n",
    "            chunk[\"_row_hash\"] = chunk.apply(hash_row, axis=1)\n",
    "\n",
    "            # Row-level validation\n",
    "            bad_idx = []\n",
    "            error_rows = []\n",
    "\n",
    "            # Detect intra-chunk duplicate PKs\n",
    "            if \"ORDERNUMBER\" in chunk.columns and \"ORDERLINENUMBER\" in chunk.columns:\n",
    "                dup_mask = chunk.duplicated(subset=[\"ORDERNUMBER\", \"ORDERLINENUMBER\"], keep=\"first\")\n",
    "                for i, is_dup in dup_mask.items():\n",
    "                    if is_dup:\n",
    "                        bad_idx.append(i)\n",
    "                        error_rows.append({\n",
    "                            \"error_type\": \"duplicate_pk\",\n",
    "                            \"ordernumber\": chunk.at[i, \"ORDERNUMBER\"],\n",
    "                            \"orderlinenumber\": chunk.at[i, \"ORDERLINENUMBER\"],\n",
    "                            \"raw_row\": chunk.loc[i].to_json(date_format='iso'),\n",
    "                            \"error_details\": \"Duplicate ordernumber+orderlinenumber in same file\"\n",
    "                        })\n",
    "\n",
    "            for i, row in chunk.iterrows():\n",
    "                if i in bad_idx:\n",
    "                    continue\n",
    "                if pd.isna(row[\"ORDERDATE\"]):\n",
    "                    bad_idx.append(i)\n",
    "                    error_rows.append({\n",
    "                        \"error_type\": \"invalid_date\",\n",
    "                        \"ordernumber\": row.get(\"ORDERNUMBER\"),\n",
    "                        \"orderlinenumber\": row.get(\"ORDERLINENUMBER\"),\n",
    "                        \"raw_row\": row.to_json(date_format='iso'),\n",
    "                        \"error_details\": \"ORDERDATE parse failure or missing\"\n",
    "                    })\n",
    "                    continue\n",
    "                try:\n",
    "                    price = float(row[\"PRICEEACH\"]) if not pd.isna(row[\"PRICEEACH\"]) else None\n",
    "                except Exception:\n",
    "                    price = None\n",
    "                if price is None or price < 0:\n",
    "                    bad_idx.append(i)\n",
    "                    error_rows.append({\n",
    "                        \"error_type\": \"invalid_price\",\n",
    "                        \"ordernumber\": row.get(\"ORDERNUMBER\"),\n",
    "                        \"orderlinenumber\": row.get(\"ORDERLINENUMBER\"),\n",
    "                        \"raw_row\": row.to_json(date_format='iso'),\n",
    "                        \"error_details\": f\"PRICEEACH invalid: {row.get('PRICEEACH')}\"\n",
    "                    })\n",
    "                    continue\n",
    "                try:\n",
    "                    qty = row.get(\"QUANTITYORDERED\")\n",
    "                    if pd.isna(qty) or int(qty) < 0:\n",
    "                        bad_idx.append(i)\n",
    "                        error_rows.append({\n",
    "                            \"error_type\": \"invalid_quantity\",\n",
    "                            \"ordernumber\": row.get(\"ORDERNUMBER\"),\n",
    "                            \"orderlinenumber\": row.get(\"ORDERLINENUMBER\"),\n",
    "                            \"raw_row\": row.to_json(date_format='iso'),\n",
    "                            \"error_details\": f\"QUANTITYORDERED invalid: {qty}\"\n",
    "                        })\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    bad_idx.append(i)\n",
    "                    error_rows.append({\n",
    "                        \"error_type\": \"invalid_quantity_parse\",\n",
    "                        \"ordernumber\": row.get(\"ORDERNUMBER\"),\n",
    "                        \"orderlinenumber\": row.get(\"ORDERLINENUMBER\"),\n",
    "                        \"raw_row\": row.to_json(date_format='iso'),\n",
    "                        \"error_details\": f\"QUANTITYORDERED parse failed: {qty}\"\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "            # Remove bad rows\n",
    "            if bad_idx:\n",
    "                bad_rows = chunk.loc[bad_idx]\n",
    "                chunk = chunk.drop(index=bad_idx)\n",
    "                rows_rejected += len(bad_rows)\n",
    "                for er in error_rows:\n",
    "                    connection.execute(text(\"\"\"\n",
    "                        INSERT INTO ingestion_errors (ingestion_id, error_type, ordernumber, orderlinenumber, raw_row, error_details)\n",
    "                        VALUES (:ingestion_id, :error_type, :ordernumber, :orderlinenumber, :raw_row, :error_details)\n",
    "                    \"\"\"), {\n",
    "                        \"ingestion_id\": ingestion_id,\n",
    "                        \"error_type\": er[\"error_type\"],\n",
    "                        \"ordernumber\": er[\"ordernumber\"],\n",
    "                        \"orderlinenumber\": er[\"orderlinenumber\"],\n",
    "                        \"raw_row\": er[\"raw_row\"],\n",
    "                        \"error_details\": er[\"error_details\"]\n",
    "                    })\n",
    "\n",
    "            # Insert good rows\n",
    "            if not chunk.empty:\n",
    "                df_to_insert = chunk.rename(columns={c: c.lower() for c in chunk.columns})\n",
    "                df_to_insert.to_sql('raw_staging_sales', con=engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "                rows_loaded += len(df_to_insert)\n",
    "\n",
    "        # Update log\n",
    "        end = datetime.datetime.utcnow()\n",
    "        connection.execute(text(\"\"\"\n",
    "            UPDATE ingestion_logs\n",
    "            SET run_completed_at = :completed,\n",
    "                rows_seen = :seen,\n",
    "                rows_loaded = :loaded,\n",
    "                rows_rejected = :rejected,\n",
    "                warnings = :warnings\n",
    "            WHERE ingestion_id = :ingestion_id\n",
    "        \"\"\"), {\n",
    "            \"completed\": end,\n",
    "            \"seen\": rows_seen,\n",
    "            \"loaded\": rows_loaded,\n",
    "            \"rejected\": rows_rejected,\n",
    "            \"warnings\": \";\".join(warnings),\n",
    "            \"ingestion_id\": ingestion_id\n",
    "        })\n",
    "\n",
    "        print(f\"Ingestion complete: seen={rows_seen} loaded={rows_loaded} rejected={rows_rejected}\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        msg = str(e.__dict__.get('orig')) if hasattr(e, '__dict__') else str(e)\n",
    "        print(\"DB error:\", msg)\n",
    "        if ingestion_id:\n",
    "            connection.execute(text(\"UPDATE ingestion_logs SET run_completed_at=NOW(), error_message=:err WHERE ingestion_id=:iid\"),\n",
    "                               {\"err\": msg, \"iid\": ingestion_id})\n",
    "        raise\n",
    "    finally:\n",
    "        connection.close()\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12825873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing set()\n",
      "extra set()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "expected = [  \"ORDERNUMBER\",\"QUANTITYORDERED\",\"PRICEEACH\",\"ORDERLINENUMBER\",\"SALES\",\n",
    "    \"ORDERDATE\",\"STATUS\",\"QTR_ID\",\"MONTH_ID\",\"YEAR_ID\",\"PRODUCTLINE\",\"MSRP\",\n",
    "    \"PRODUCTCODE\",\"CUSTOMERNAME\",\"PHONE\",\"ADDRESSLINE1\",\"ADDRESSLINE2\",\n",
    "    \"CITY\",\"STATE\",\"POSTALCODE\",\"COUNTRY\",\"TERRITORY\",\"CONTACTLASTNAME\",\n",
    "    \"CONTACTFIRSTNAME\",\"DEALSIZE\" ]  # same EXPECTED_COLS list\n",
    "df = pd.read_csv(\"Data1/sales_data_sample.csv\", nrows=0, encoding=\"latin1\")\n",
    "cols = [c.strip() for c in df.columns]\n",
    "missing = set(expected) - set(cols)\n",
    "extra = set(cols) - set(expected)\n",
    "print(\"missing\", missing)\n",
    "print(\"extra\", extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac21e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df = pd.read_csv(\"Data1/sales_data_sample.csv\", parse_dates=['ORDERDATE'], dayfirst=False, keep_default_na=False, encoding=\"latin1\")\n",
    "\n",
    "# Filter bad dates\n",
    "bad_dates = df[df['ORDERDATE'].isna()]\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save the bad rows\n",
    "bad_dates.to_csv(\"Data1/bad_orderdate_rows.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7444e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSRP  PRICEEACH  quantityordered  cost_each  profit_est  discount_pct\n",
      "0    95      95.70               30       57.0     1161.00     -0.007368\n",
      "1    95      81.35               34       57.0      827.90      0.143684\n",
      "2    95      94.74               41       57.0     1547.34      0.002737\n",
      "3    95      83.26               45       57.0     1181.70      0.123579\n",
      "4    95     100.00               49       57.0     2107.00     -0.052632\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset from your CSV file\n",
    "df = pd.read_csv(\"Data1/sales_data_sample.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Convert numeric fields\n",
    "df['msrp'] = pd.to_numeric(df['MSRP'], errors='coerce')\n",
    "df['priceeach'] = pd.to_numeric(df['PRICEEACH'], errors='coerce')\n",
    "df['quantityordered'] = pd.to_numeric(df['QUANTITYORDERED'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Derived fields\n",
    "df['cost_each'] = df['msrp'].fillna(0) * 0.6   # assume cost = 60% of MSRP\n",
    "df['profit_est'] = (df['priceeach'] - df['cost_each']) * df['quantityordered']\n",
    "df['discount_pct'] = np.where(df['msrp'] > 0, (df['msrp'] - df['priceeach']) / df['msrp'], 0)\n",
    "\n",
    "# Preview\n",
    "print(df[['MSRP','PRICEEACH','quantityordered','cost_each','profit_est','discount_pct']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775e70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
